{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(420)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Batch Gradient Descent\n",
    "def batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2, 1)\n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
    "        theta = theta - learning_rate * gradients\n",
    "    return theta\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "def stochastic_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n",
    "    m = len(y)\n",
    "    theta = np.random.randn(2, 1)\n",
    "    for iteration in range(n_iterations):\n",
    "        for i in range(m):\n",
    "            random_index = np.random.randint(m)\n",
    "            xi = X[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            gradients = xi.T.dot(xi.dot(theta) - yi)\n",
    "            theta = theta - learning_rate * gradients\n",
    "    return theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding bias term to X\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "theta_batch = batch_gradient_descent(X_b, y)\n",
    "print(\"Batch Gradient Descent: Intercept and Coefficient\")\n",
    "print(theta_batch)\n",
    "\n",
    "theta_stochastic = stochastic_gradient_descent(X_b, y)\n",
    "print(\"Stochastic Gradient Descent: Intercept and Coefficient\")\n",
    "print(theta_stochastic)\n",
    "\n",
    "# Plot the data and regression lines\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, X_b.dot(theta_batch), 'r-', label='Batch Gradient Descent')\n",
    "plt.plot(X, X_b.dot(theta_stochastic), 'g-', label='Stochastic Gradient Descent')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (4 + 3 * X + np.random.rand(100, 1)) > 6\n",
    "\n",
    "# Separate data into two classes\n",
    "X_class1 = X[y[:, 0] == False]\n",
    "X_class2 = X[y[:, 0] == True]\n",
    "\n",
    "# Create a decision boundary\n",
    "x_boundary = np.linspace(0, 2, 100)\n",
    "y_boundary = 4 + 3 * x_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and decision boundary\n",
    "plt.scatter(X_class1, [0] * len(X_class1), marker='o', label='Class 1', color='b')\n",
    "plt.scatter(X_class2, [0] * len(X_class2), marker='x', label='Class 2', color='r')\n",
    "plt.plot(x_boundary, y_boundary, 'k-', label='Decision Boundary')\n",
    "plt.xlabel('X')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a logistic regression model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X, y)\n",
    "\n",
    "# Create a decision boundary\n",
    "x_boundary = np.linspace(0, 2, 100)\n",
    "y_proba = model.predict_proba(x_boundary.reshape(-1, 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and decision boundary\n",
    "plt.scatter(X[y[:, 0] == 0], y[y[:, 0] == 0], marker='o', label='Class 0', color='b')\n",
    "plt.scatter(X[y[:, 0] == 1], y[y[:, 0] == 1], marker='x', label='Class 1', color='r')\n",
    "plt.plot(x_boundary, y_proba, 'k-', label='Decision Boundary')\n",
    "plt.xlabel('X')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
    "\n",
    "# Create and fit a k-NN classifier with k=5\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Create a meshgrid for decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points and decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], marker='o', label='Class 0', color='b')\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], marker='x', label='Class 1', color='r')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LWR model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Predictions with LWR\n",
    "y_pred = np.zeros_like(y)\n",
    "for i, x in enumerate(X):\n",
    "    weights = np.exp(-((X - x) ** 2) / (2 * 0.1 ** 2))\n",
    "    model.fit(X, y, sample_weight=weights)\n",
    "    y_pred[i] = model.predict(x.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and LWR predictions\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_pred, color='navy', label='LWR')\n",
    "plt.legend()\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Locally Weighted Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X, y = datasets.make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Create an SVM model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Fit the model to the data\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mesh grid for plotting\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 50),\n",
    "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 50))\n",
    "\n",
    "# Predict the class for each point in the mesh\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points and decision boundary\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=40)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Support Vector Machine (SVM) Classification')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
